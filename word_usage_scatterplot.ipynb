{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abb0d082",
   "metadata": {},
   "source": [
    "# Word Usage Analysis: Scatter Plot of Age-Specific Words\n",
    "\n",
    "This notebook creates a scatter plot visualization of distinctive words used by young vs. old speakers, similar to the visualization shown in the provided image. The analysis uses data from the BNC2014 corpus.\n",
    "\n",
    "We'll use the same data processing pipeline from the binary age classifier but focus on creating a scattertext visualization that shows:\n",
    "1. Words more frequently used by young speakers\n",
    "2. Words more frequently used by old speakers \n",
    "3. Words used by both groups but not distinctively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b48eb50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jubal\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "import lxml.etree as ET\n",
    "\n",
    "# For visualization\n",
    "import scattertext as st\n",
    "import spacy\n",
    "import html\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set(font_scale=1.2)\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Install scattertext if not already installed\n",
    "try:\n",
    "    import scattertext\n",
    "except ImportError:\n",
    "    !pip install scattertext\n",
    "    import scattertext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87db6c9",
   "metadata": {},
   "source": [
    "## 1. Load the BNC2014 Corpus Data and Speaker Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaf437e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded metadata for 671 speakers\n"
     ]
    }
   ],
   "source": [
    "# Set the path to the dataset\n",
    "path = 'Dataset'  \n",
    "dir_corpus = os.path.join(path, 'spoken', 'tagged')\n",
    "dir_meta = os.path.join(path, 'spoken', 'metadata')\n",
    "\n",
    "# Load speaker metadata\n",
    "fields_s = pd.read_csv(\n",
    "    os.path.join(dir_meta, 'metadata-fields-speaker.txt'),\n",
    "    sep='\\t', skiprows=1, index_col=0\n",
    ")\n",
    "\n",
    "# Load the speaker metadata\n",
    "df_speakers_meta = pd.read_csv(\n",
    "    os.path.join(dir_meta, 'bnc2014spoken-speakerdata.tsv'),\n",
    "    sep='\\t', names=fields_s['XML tag'], index_col=0\n",
    ")\n",
    "\n",
    "print(f\"Loaded metadata for {len(df_speakers_meta)} speakers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25f1149d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of speakers by binary age classification:\n",
      "binary_age\n",
      "Old      363\n",
      "Young    299\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Function to map BNC age ranges to binary categories\n",
    "def map_to_binary_age(age_range):\n",
    "    \"\"\"\n",
    "    Map BNC age ranges to binary categories:\n",
    "    Young (0-29) vs Old (30+)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    age_range : str\n",
    "        Age range from BNC metadata (e.g., '0_18', '19-29', '30_59', '60_plus')\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        'Young' or 'Old' classification\n",
    "    \"\"\"\n",
    "    if pd.isna(age_range) or age_range == 'Unknown':\n",
    "        return np.nan\n",
    "    \n",
    "    # Handle different formats in the age range field\n",
    "    try:\n",
    "        # Extract the upper bound of the age range\n",
    "        if '_' in str(age_range):\n",
    "            ages = str(age_range).split('_')\n",
    "        elif '-' in str(age_range):\n",
    "            ages = str(age_range).split('-')\n",
    "        else:\n",
    "            # Silently skip unknown formats without printing\n",
    "            return np.nan\n",
    "        \n",
    "        # Parse the upper bound\n",
    "        if ages[1] == 'plus':\n",
    "            upper = 100  # Arbitrarily high for '60_plus'\n",
    "        else:\n",
    "            upper = int(ages[1])\n",
    "        \n",
    "        # Classify as young or old\n",
    "        if upper <= 29:\n",
    "            return \"Young\"\n",
    "        else:\n",
    "            return \"Old\"\n",
    "    except Exception as e:\n",
    "        # Silently return np.nan for any errors\n",
    "        return np.nan\n",
    "\n",
    "# Apply the binary age classification to speaker metadata\n",
    "df_speakers_meta['binary_age'] = df_speakers_meta['agerange'].apply(map_to_binary_age)\n",
    "\n",
    "# Display the counts for each binary age group\n",
    "binary_age_counts = df_speakers_meta['binary_age'].value_counts()\n",
    "print(\"Distribution of speakers by binary age classification:\")\n",
    "print(binary_age_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e750f33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1/30: S23A-tgd.xml\n",
      "Processing file 6/30: S26N-tgd.xml\n",
      "Processing file 11/30: S2A5-tgd.xml\n",
      "Processing file 16/30: S2CY-tgd.xml\n",
      "Processing file 21/30: S2FT-tgd.xml\n",
      "Processing file 26/30: S2K6-tgd.xml\n",
      "\n",
      "Loaded 272258 word tokens from 30 files\n",
      "Found 64 unique speakers in the processed data\n"
     ]
    }
   ],
   "source": [
    "# Process tagged corpus files to extract word and linguistic feature data\n",
    "# We'll limit to 30 files to keep processing reasonable, but you can increase this\n",
    "file_limit = 30  # Adjust based on your computational resources\n",
    "\n",
    "tagged_rows = []\n",
    "try:\n",
    "    # Load a subset of corpus files\n",
    "    for file_count, fname in enumerate(sorted(os.listdir(dir_corpus))[:file_limit]):\n",
    "        if file_count % 5 == 0:\n",
    "            print(f\"Processing file {file_count+1}/{file_limit}: {fname}\")\n",
    "            \n",
    "        fpath = os.path.join(dir_corpus, fname)\n",
    "        xml = ET.parse(fpath)\n",
    "        root = xml.getroot()\n",
    "        text_id = root.get('id')\n",
    "        \n",
    "        for u in root.findall('.//u'):\n",
    "            utt_id = u.get('n')\n",
    "            spk = u.get('who')\n",
    "            for w in u.findall('w'):\n",
    "                tagged_rows.append({\n",
    "                    'text_id': text_id,\n",
    "                    'utterance_id': utt_id,\n",
    "                    'speaker_id': spk,\n",
    "                    'word': w.text,\n",
    "                    'lemma': w.get('lemma'),\n",
    "                    'pos': w.get('pos'),\n",
    "                    'class': w.get('class'),\n",
    "                    'usas': w.get('usas'),\n",
    "                })\n",
    "    \n",
    "    # Create a DataFrame from the extracted data\n",
    "    df_tagged = pd.DataFrame(tagged_rows)\n",
    "    \n",
    "    print(f\"\\nLoaded {len(df_tagged)} word tokens from {file_limit} files\")\n",
    "    print(f\"Found {df_tagged['speaker_id'].nunique()} unique speakers in the processed data\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading corpus data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8f9865f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Of 64 speakers in the corpus data, 61 have valid age data\n",
      "Filtered corpus data contains 271916 word tokens from 61 speakers\n"
     ]
    }
   ],
   "source": [
    "# Count of speakers with valid age data\n",
    "valid_age_speakers = set(df_speakers_meta[~df_speakers_meta['binary_age'].isna()].index)\n",
    "tagged_speakers = set(df_tagged['speaker_id'].unique())\n",
    "valid_speakers = valid_age_speakers.intersection(tagged_speakers)\n",
    "\n",
    "print(f\"\\nOf {len(tagged_speakers)} speakers in the corpus data, {len(valid_speakers)} have valid age data\")\n",
    "\n",
    "# Filter to only include speakers with valid age data\n",
    "df_tagged_valid = df_tagged[df_tagged['speaker_id'].isin(valid_speakers)]\n",
    "print(f\"Filtered corpus data contains {len(df_tagged_valid)} word tokens from {len(valid_speakers)} speakers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345622fe",
   "metadata": {},
   "source": [
    "## 2. Create Corpus for ScatterText Visualization\n",
    "\n",
    "Now, let's create a corpus suitable for the ScatterText visualization by combining all utterances from each speaker into a single document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2cf7fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created corpus with 60 documents\n",
      "Age group distribution in corpus:\n",
      "age_group\n",
      "Old      34\n",
      "Young    26\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample documents:\n",
      "\n",
      "Sample Young document:\n",
      "exactly never mind exactly or we yeah yeah yeah yeah exactly and that 's like we go months and months and months without seeing each other so like we saw each other in December and that was the first ...\n",
      "\n",
      "Sample Old document:\n",
      "oh right yeah yeah he put it on the system did he ? yeah you 'll have one he 'll have one do n't walk away --ANONnameF cor did they ? did n't they ? oh my goodness are you alright though ? okay three ...\n"
     ]
    }
   ],
   "source": [
    "# Combine all words from each speaker into a single document\n",
    "# We'll use a slightly different approach optimized for scattertext visualization\n",
    "\n",
    "# Create a list to store documents with metadata\n",
    "documents = []\n",
    "\n",
    "# Group by speaker and create one document per speaker\n",
    "for speaker_id, speaker_data in df_tagged_valid.groupby('speaker_id'):\n",
    "    if speaker_id in df_speakers_meta.index:\n",
    "        # Get the age group for this speaker\n",
    "        age_group = df_speakers_meta.loc[speaker_id, 'binary_age']\n",
    "        if pd.isna(age_group):\n",
    "            continue\n",
    "        \n",
    "        # Extract words and combine into a text\n",
    "        words = speaker_data['word'].fillna('').tolist()\n",
    "        text = ' '.join([w for w in words if w])\n",
    "        \n",
    "        # Keep only documents with sufficient length\n",
    "        if len(text.split()) >= 50:\n",
    "            documents.append({\n",
    "                'speaker_id': speaker_id,\n",
    "                'age_group': age_group,\n",
    "                'text': text\n",
    "            })\n",
    "\n",
    "# Create a DataFrame from the documents\n",
    "corpus_df = pd.DataFrame(documents)\n",
    "\n",
    "print(f\"Created corpus with {len(corpus_df)} documents\")\n",
    "print(f\"Age group distribution in corpus:\")\n",
    "print(corpus_df['age_group'].value_counts())\n",
    "\n",
    "# Sample a few examples\n",
    "print(\"\\nSample documents:\")\n",
    "for age_group in ['Young', 'Old']:\n",
    "    print(f\"\\nSample {age_group} document:\")\n",
    "    sample = corpus_df[corpus_df['age_group'] == age_group].sample(1).iloc[0]\n",
    "    print(sample['text'][:200] + \"...\")  # Show first 200 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1a6c2c",
   "metadata": {},
   "source": [
    "## 3. Create Scattertext Visualization\n",
    "\n",
    "Now we'll use the scattertext library to create an interactive HTML visualization of word usage differences between age groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "196c65b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation maximum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 23\u001b[0m\n\u001b[0;32m     15\u001b[0m corpus \u001b[38;5;241m=\u001b[39m st\u001b[38;5;241m.\u001b[39mCorpusFromPandas(\n\u001b[0;32m     16\u001b[0m     corpus_df,\n\u001b[0;32m     17\u001b[0m     category_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage_group\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     18\u001b[0m     text_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_text\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     19\u001b[0m     nlp\u001b[38;5;241m=\u001b[39mnlp\n\u001b[0;32m     20\u001b[0m )\u001b[38;5;241m.\u001b[39mbuild()\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Create the scattertext visualization\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m html_file \u001b[38;5;241m=\u001b[39m \u001b[43mst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproduce_scattertext_explorer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOld\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Top category\u001b[39;49;00m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategory_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOld\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnot_category_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mYoung\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwidth_in_pixels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspeaker_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mminimum_term_frequency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mterm_significance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmann_whitney\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use Mann-Whitney U test for significance\u001b[39;49;00m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mScalers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense_rank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# Use dense rank for better visualization\u001b[39;49;00m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_docs_per_category\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# Limit number of documents for faster rendering\u001b[39;49;00m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_non_text_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# Include non-text features if available\u001b[39;49;00m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpmi_threshold_coefficient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Higher value: more focused on distinctive terms\u001b[39;49;00m\n\u001b[0;32m     36\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Save the visualization to an HTML file\u001b[39;00m\n\u001b[0;32m     39\u001b[0m output_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage_scattertext.html\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\scattertext\\__init__.py:675\u001b[0m, in \u001b[0;36mproduce_scattertext_explorer\u001b[1;34m(corpus, category, category_name, not_category_name, protocol, pmi_threshold_coefficient, minimum_term_frequency, minimum_not_category_term_frequency, max_terms, filter_unigrams, height_in_pixels, width_in_pixels, max_snippets, max_docs_per_category, metadata, scores, x_coords, y_coords, original_x, original_y, rescale_x, rescale_y, singleScoreMode, sort_by_dist, reverse_sort_scores_for_not_category, use_full_doc, transform, jitter, gray_zero_scores, term_ranker, asian_mode, match_full_line, use_non_text_features, show_top_terms, show_characteristic, word_vec_use_p_vals, max_p_val, p_value_colors, term_significance, save_svg_button, x_label, y_label, d3_url, d3_scale_chromatic_url, pmi_filter_thresold, alternative_text_field, terms_to_include, semiotic_square, num_terms_semiotic_square, not_categories, neutral_categories, extra_categories, show_neutral, neutral_category_name, get_tooltip_content, x_axis_values, y_axis_values, x_axis_values_format, y_axis_values_format, color_func, term_scorer, term_scorer_kwargs, show_axes, show_axes_and_cross_hairs, show_diagonal, use_global_scale, horizontal_line_y_position, vertical_line_x_position, show_cross_axes, show_extra, extra_category_name, censor_points, center_label_over_points, x_axis_labels, y_axis_labels, topic_model_term_lists, topic_model_preview_size, metadata_descriptions, vertical_lines, characteristic_scorer, term_colors, unified_context, show_category_headings, highlight_selected_category, include_term_category_counts, div_name, alternative_term_func, term_metadata, term_metadata_df, max_overlapping, include_all_contexts, show_corpus_stats, sort_doc_labels_by_name, enable_term_category_description, always_jump, get_custom_term_html, header_names, header_sorting_algos, ignore_categories, d3_color_scale, background_labels, tooltip_columns, tooltip_column_names, term_description_columns, term_description_column_names, term_word_in_term_description, color_column, color_score_column, label_priority_column, text_color_column, suppress_text_column, background_color, left_list_column, censor_point_column, right_order_column, line_coordinates, subword_encoding, top_terms_length, top_terms_left_buffer, dont_filter, use_offsets, get_column_header_html, show_term_etc, sort_contexts_by_meta, show_chart, return_data, suppress_circles, return_scatterplot_structure)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m semiotic_square:\n\u001b[0;32m    673\u001b[0m     html_base \u001b[38;5;241m=\u001b[39m get_semiotic_square_html(num_terms_semiotic_square,\n\u001b[0;32m    674\u001b[0m                                          semiotic_square)\n\u001b[1;32m--> 675\u001b[0m scatter_chart_data \u001b[38;5;241m=\u001b[39m \u001b[43mscatter_chart_explorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategory_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategory_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnot_category_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnot_category_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnot_categories\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnot_categories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_docs_per_category\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_docs_per_category\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mcallable\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43malternative_text_field\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malternative_text_field\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mneutral_category_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneutral_category_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_category_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_category_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mneutral_categories\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneutral_categories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_categories\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_categories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackground_scorer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcharacteristic_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_term_category_counts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_term_category_counts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_offsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_offsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m line_coordinates \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    695\u001b[0m     scatter_chart_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mline\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m line_coordinates\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\scattertext\\ScatterChartExplorer.py:113\u001b[0m, in \u001b[0;36mScatterChartExplorer.to_dict\u001b[1;34m(self, category, category_name, not_category_name, scores, metadata, max_docs_per_category, transform, alternative_text_field, title_case_names, not_categories, neutral_categories, extra_categories, neutral_category_name, extra_category_name, background_scorer, include_term_category_counts, use_offsets, **kwargs)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m {} \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[0;32m    111\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcessive arguments passed to ScatterChartExplorer.to_dict: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(kwargs))\n\u001b[1;32m--> 113\u001b[0m json_data \u001b[38;5;241m=\u001b[39m \u001b[43mScatterChart\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mcategory_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategory_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mnot_category_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnot_category_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mscores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mtitle_case_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtitle_case_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mnot_categories\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnot_categories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mneutral_categories\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneutral_categories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mextra_categories\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_categories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mbackground_scorer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackground_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43muse_offsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_offsets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m docs_getter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_docs_getter(max_docs_per_category, alternative_text_field)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m neutral_category_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\scattertext\\ScatterChart.py:293\u001b[0m, in \u001b[0;36mScatterChart.to_dict\u001b[1;34m(self, category, category_name, not_category_name, scores, transform, title_case_names, not_categories, neutral_categories, extra_categories, background_scorer, use_offsets, **kwargs)\u001b[0m\n\u001b[0;32m    291\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_x_and_y_coords_to_term_df_if_injected(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_term_category_frequencies())\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scores \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 293\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_default_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnot_categories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    295\u001b[0m category_column_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(category) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m freq\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    296\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory score\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m CornerScore\u001b[38;5;241m.\u001b[39mget_scores_for_category(\n\u001b[0;32m    297\u001b[0m     df[category_column_name],\n\u001b[0;32m    298\u001b[0m     df[[\u001b[38;5;28mstr\u001b[39m(c) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m freq\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m not_categories]]\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    299\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\scattertext\\ScatterChart.py:586\u001b[0m, in \u001b[0;36mScatterChart._get_default_scores\u001b[1;34m(self, category, other_categories, df)\u001b[0m\n\u001b[0;32m    584\u001b[0m not_cat_word_counts \u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;28mstr\u001b[39m(c) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m freq\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m other_categories]]\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    585\u001b[0m \u001b[38;5;66;03m# scores = ScaledFScore.get_scores(cat_word_counts, not_cat_word_counts)\u001b[39;00m\n\u001b[1;32m--> 586\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mRankDifference\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcat_word_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnot_cat_word_counts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\scattertext\\termscoring\\RankDifference.py:8\u001b[0m, in \u001b[0;36mRankDifference.get_scores\u001b[1;34m(self, a, b)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_scores\u001b[39m(\u001b[38;5;28mself\u001b[39m, a, b):\n\u001b[1;32m----> 8\u001b[0m \tto_ret \u001b[38;5;241m=\u001b[39m (rankdata(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdense\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrankdata\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdense\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \t          \u001b[38;5;241m-\u001b[39m rankdata(b, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdense\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(rankdata(b, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdense\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[0;32m     11\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(a) \u001b[38;5;241m==\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries:\n\u001b[0;32m     12\u001b[0m \t\t\u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mSeries(to_ret, index\u001b[38;5;241m=\u001b[39ma\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\numpy\\core\\fromnumeric.py:2810\u001b[0m, in \u001b[0;36mmax\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2692\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_max_dispatcher)\n\u001b[0;32m   2693\u001b[0m \u001b[38;5;129m@set_module\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   2694\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmax\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[0;32m   2695\u001b[0m          where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[0;32m   2696\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2697\u001b[0m \u001b[38;5;124;03m    Return the maximum of an array or maximum along an axis.\u001b[39;00m\n\u001b[0;32m   2698\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2808\u001b[0m \u001b[38;5;124;03m    5\u001b[39;00m\n\u001b[0;32m   2809\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2811\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\numpy\\core\\fromnumeric.py:88\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[1;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ufunc\u001b[38;5;241m.\u001b[39mreduce(obj, axis, dtype, out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n",
      "\u001b[1;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
     ]
    }
   ],
   "source": [
    "# Load spacy model - we need this for text preprocessing\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "except:\n",
    "    # If the model is not available, download it\n",
    "    import subprocess\n",
    "    subprocess.run(['python', '-m', 'spacy', 'download', 'en_core_web_sm'])\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Process texts with spaCy for better parsing\n",
    "corpus_df['processed_text'] = corpus_df['text'].apply(lambda x: ' '.join([token.lemma_ for token in nlp(x) \n",
    "                                                                          if not token.is_stop and not token.is_punct]))\n",
    "\n",
    "# Create a corpus for scattertext\n",
    "corpus = st.CorpusFromPandas(\n",
    "    corpus_df,\n",
    "    category_col='age_group',\n",
    "    text_col='processed_text',\n",
    "    nlp=nlp\n",
    ").build()\n",
    "\n",
    "# Create the scattertext visualization\n",
    "html_file = st.produce_scattertext_explorer(\n",
    "    corpus,\n",
    "    category='Old',  # Top category\n",
    "    category_name='Old',\n",
    "    not_category_name='Young',\n",
    "    width_in_pixels=1000,\n",
    "    metadata=corpus_df['speaker_id'],\n",
    "    minimum_term_frequency=5,\n",
    "    term_significance='mann_whitney',  # Use Mann-Whitney U test for significance\n",
    "    transform=st.Scalers.dense_rank,   # Use dense rank for better visualization\n",
    "    max_docs_per_category=100,          # Limit number of documents for faster rendering\n",
    "    use_non_text_features=True,         # Include non-text features if available\n",
    "    pmi_threshold_coefficient=4,        # Higher value: more focused on distinctive terms\n",
    ")\n",
    "\n",
    "# Save the visualization to an HTML file\n",
    "output_filename = 'age_scattertext.html'\n",
    "with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write(html_file)\n",
    "\n",
    "print(f\"Saved interactive visualization to {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51104ae2",
   "metadata": {},
   "source": [
    "## 4. Create Static Scatter Plot for Age-Specific Words\n",
    "\n",
    "Let's also create a static scatter plot using matplotlib and seaborn that shows the most distinctive words for each age group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c2b9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CountVectorizer to get word frequencies\n",
    "count_vectorizer = CountVectorizer(\n",
    "    min_df=5,          # Minimum document frequency\n",
    "    max_df=0.7,        # Maximum document frequency (remove very common words)\n",
    "    stop_words='english', # Remove English stopwords\n",
    "    max_features=1000  # Limit to top 1000 features\n",
    ")\n",
    "\n",
    "# Fit and transform the corpus\n",
    "X_counts = count_vectorizer.fit_transform(corpus_df['processed_text'])\n",
    "words = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame with word counts by category\n",
    "word_counts = pd.DataFrame(X_counts.toarray(), columns=words)\n",
    "word_counts['age_group'] = corpus_df['age_group'].values\n",
    "\n",
    "# Calculate average frequency for each word by age group\n",
    "young_freqs = word_counts[word_counts['age_group'] == 'Young'].drop('age_group', axis=1).mean()\n",
    "old_freqs = word_counts[word_counts['age_group'] == 'Old'].drop('age_group', axis=1).mean()\n",
    "\n",
    "# Combine frequencies into a DataFrame\n",
    "word_freq_df = pd.DataFrame({\n",
    "    'word': words,\n",
    "    'young_freq': young_freqs.values,\n",
    "    'old_freq': old_freqs.values\n",
    "})\n",
    "\n",
    "# Add a small constant to avoid log(0)\n",
    "epsilon = 1e-10\n",
    "word_freq_df['young_freq_adj'] = word_freq_df['young_freq'] + epsilon\n",
    "word_freq_df['old_freq_adj'] = word_freq_df['old_freq'] + epsilon\n",
    "\n",
    "# Calculate log ratio as a measure of distinctiveness\n",
    "word_freq_df['log_ratio'] = np.log2(word_freq_df['old_freq_adj'] / word_freq_df['young_freq_adj'])\n",
    "\n",
    "# Calculate overall frequency (for point size)\n",
    "word_freq_df['total_freq'] = word_freq_df['young_freq'] + word_freq_df['old_freq']\n",
    "\n",
    "# Identify most distinctive words for each category\n",
    "young_words = word_freq_df.sort_values('log_ratio').head(30)['word'].tolist()\n",
    "old_words = word_freq_df.sort_values('log_ratio', ascending=False).head(30)['word'].tolist()\n",
    "distinctive_words = young_words + old_words\n",
    "\n",
    "# Filter to most distinctive words for visualization\n",
    "plot_df = word_freq_df[word_freq_df['word'].isin(distinctive_words)].copy()\n",
    "\n",
    "# Create a static scatter plot\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Set color based on log_ratio (blue for Young, red for Old)\n",
    "colors = ['blue' if ratio < 0 else 'red' for ratio in plot_df['log_ratio']]\n",
    "\n",
    "# Create scatter plot\n",
    "scatter = plt.scatter(\n",
    "    x=plot_df['young_freq'],\n",
    "    y=plot_df['old_freq'],\n",
    "    s=plot_df['total_freq'] * 1000,  # Scale point size\n",
    "    alpha=0.6,\n",
    "    c=colors\n",
    ")\n",
    "\n",
    "# Add labels for distinctive words\n",
    "for i, row in plot_df.iterrows():\n",
    "    plt.annotate(\n",
    "        row['word'], \n",
    "        (row['young_freq'], row['old_freq']),\n",
    "        fontsize=10,\n",
    "        ha='center' if abs(row['log_ratio']) < 1 else ('right' if row['log_ratio'] < 0 else 'left'),\n",
    "        va='center',\n",
    "        weight='bold' if abs(row['log_ratio']) > 2 else 'normal'\n",
    "    )\n",
    "\n",
    "# Add diagonal line\n",
    "max_val = max(plot_df['young_freq'].max(), plot_df['old_freq'].max()) * 1.1\n",
    "plt.plot([0, max_val], [0, max_val], 'k--', alpha=0.3)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Frequency in Young Speaker Texts', fontsize=14)\n",
    "plt.ylabel('Frequency in Old Speaker Texts', fontsize=14)\n",
    "plt.title('Word Usage Patterns: Young vs. Old Speakers', fontsize=16)\n",
    "\n",
    "# Add legend\n",
    "blue_patch = plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='blue', markersize=10, label='More common in Young')\n",
    "red_patch = plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=10, label='More common in Old')\n",
    "plt.legend(handles=[blue_patch, red_patch], loc='upper left')\n",
    "\n",
    "# Improve layout\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('age_word_usage_scatter.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"Scatter plot created and saved as 'age_word_usage_scatter.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0411dc34",
   "metadata": {},
   "source": [
    "## 5. Word Frequency Analysis: Create Top Words Tables\n",
    "\n",
    "Let's also create tables showing the top words distinctive of each age group, along with their frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6343c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a modified scatter plot that more clearly shows words associated with each class\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "tfidf_vec = TfidfVectorizer(\n",
    "    min_df=5,          # Minimum document frequency\n",
    "    max_df=0.7,        # Maximum document frequency (remove very common words)\n",
    "    stop_words='english', # Remove English stopwords\n",
    "    max_features=3000  # Limit to top 3000 features to keep processing manageable\n",
    ")\n",
    "\n",
    "# Fit vectorizer on all texts\n",
    "X_tfidf = tfidf_vec.fit_transform(corpus_df['processed_text'])\n",
    "\n",
    "# Get feature names\n",
    "feature_names = tfidf_vec.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame with TF-IDF values for each document\n",
    "tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=feature_names)\n",
    "tfidf_df['age_group'] = corpus_df['age_group'].values\n",
    "\n",
    "# Calculate average TF-IDF value for each term by age group\n",
    "young_tfidf = tfidf_df[tfidf_df['age_group'] == 'Young'].drop('age_group', axis=1).mean()\n",
    "old_tfidf = tfidf_df[tfidf_df['age_group'] == 'Old'].drop('age_group', axis=1).mean()\n",
    "\n",
    "# Create a DataFrame of term importances\n",
    "term_importances = pd.DataFrame({\n",
    "    'word': feature_names,\n",
    "    'young_importance': young_tfidf,\n",
    "    'old_importance': old_tfidf\n",
    "})\n",
    "\n",
    "# Add a small constant to avoid division by zero\n",
    "epsilon = 1e-8\n",
    "term_importances['ratio'] = (term_importances['old_importance'] + epsilon) / \\\n",
    "                            (term_importances['young_importance'] + epsilon)\n",
    "term_importances['log_ratio'] = np.log2(term_importances['ratio'])\n",
    "\n",
    "# Find the most distinctive terms for each age group\n",
    "young_words = term_importances.nlargest(40, 'young_importance')\n",
    "young_distinctive = term_importances.nsmallest(40, 'log_ratio')\n",
    "old_words = term_importances.nlargest(40, 'old_importance')\n",
    "old_distinctive = term_importances.nlargest(40, 'log_ratio')\n",
    "\n",
    "# Take a union of the most important terms for visualization\n",
    "important_terms = pd.concat([young_distinctive, old_distinctive]).drop_duplicates('word')\n",
    "\n",
    "# Create a new figure\n",
    "plt.figure(figsize=(14, 12))\n",
    "\n",
    "# Plot the terms with distinct coloring based on which age group they're associated with\n",
    "for _, row in important_terms.iterrows():\n",
    "    x = row['young_importance']\n",
    "    y = row['old_importance']\n",
    "    word = row['word']\n",
    "    # Color words based on their association\n",
    "    if row['log_ratio'] < -1:  # Strongly associated with Young\n",
    "        color = 'blue'\n",
    "        alpha = 0.8\n",
    "    elif row['log_ratio'] > 1:  # Strongly associated with Old\n",
    "        color = 'red'\n",
    "        alpha = 0.8\n",
    "    else:  # Less strongly associated\n",
    "        color = 'purple'\n",
    "        alpha = 0.5\n",
    "    \n",
    "    # Vary sizes based on total importance (how \"special\" the word is)\n",
    "    size = 10 + 1000 * (x + y)\n",
    "    plt.scatter(x, y, s=size, color=color, alpha=alpha)\n",
    "    \n",
    "    # Add word labels\n",
    "    plt.annotate(\n",
    "        word,\n",
    "        (x, y),\n",
    "        fontsize=11,\n",
    "        ha='center' if abs(row['log_ratio']) < 0.5 else ('right' if row['log_ratio'] < 0 else 'left'),\n",
    "        va='center',\n",
    "        alpha=0.9,\n",
    "        weight='bold' if abs(row['log_ratio']) > 1.5 else 'normal'\n",
    "    )\n",
    "\n",
    "# Add diagonal line\n",
    "max_val = max(important_terms['young_importance'].max(), important_terms['old_importance'].max()) * 1.1\n",
    "plt.plot([0, max_val], [0, max_val], 'k--', alpha=0.3)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Association with Young Speakers', fontsize=14)\n",
    "plt.ylabel('Association with Old Speakers', fontsize=14)\n",
    "plt.title('Words Associated with Young vs. Old Speakers', fontsize=16)\n",
    "\n",
    "# Add legend\n",
    "blue_patch = plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='blue', markersize=10, label='Young')\n",
    "red_patch = plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=10, label='Old')\n",
    "purple_patch = plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='purple', markersize=10, label='Shared')\n",
    "plt.legend(handles=[blue_patch, red_patch, purple_patch], loc='upper left')\n",
    "\n",
    "# Improve layout\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save and show\n",
    "plt.savefig('age_specific_words_scatter.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nScatter plot of age-specific words created and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ba0944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate significance using Mann-Whitney U test\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# Calculate p-values for each term\n",
    "p_values = []\n",
    "for word in words:\n",
    "    young_word_counts = word_counts[word_counts['age_group'] == 'Young'][word]\n",
    "    old_word_counts = word_counts[word_counts['age_group'] == 'Old'][word]\n",
    "    \n",
    "    try:\n",
    "        u_stat, p_value = mannwhitneyu(young_word_counts, old_word_counts, alternative='two-sided')\n",
    "        p_values.append(p_value)\n",
    "    except:\n",
    "        p_values.append(1.0)  # If test fails, assign no significance\n",
    "\n",
    "# Add p-values to the DataFrame\n",
    "word_freq_df['p_value'] = p_values\n",
    "\n",
    "# Add significance indicator\n",
    "word_freq_df['significant'] = word_freq_df['p_value'] < 0.05\n",
    "\n",
    "# Filter for significant differences only\n",
    "sig_words_df = word_freq_df[word_freq_df['significant']].copy()\n",
    "\n",
    "# Add a distinctiveness category\n",
    "sig_words_df['distinctiveness'] = pd.cut(\n",
    "    sig_words_df['log_ratio'],\n",
    "    bins=[-float('inf'), -1, 1, float('inf')],\n",
    "    labels=['Young', 'Similar', 'Old']\n",
    ")\n",
    "\n",
    "# Get the top words for each category\n",
    "young_distinctive = sig_words_df[sig_words_df['distinctiveness'] == 'Young'].sort_values('log_ratio').head(20)\n",
    "old_distinctive = sig_words_df[sig_words_df['distinctiveness'] == 'Old'].sort_values('log_ratio', ascending=False).head(20)\n",
    "\n",
    "# Display tables of top distinctive words\n",
    "print(\"Top 20 words distinctively used by YOUNG speakers:\")\n",
    "print(\"=\"*50)\n",
    "young_table = young_distinctive[['word', 'young_freq', 'old_freq', 'log_ratio', 'p_value']].reset_index(drop=True)\n",
    "print(young_table)\n",
    "\n",
    "print(\"\\nTop 20 words distinctively used by OLD speakers:\")\n",
    "print(\"=\"*50)\n",
    "old_table = old_distinctive[['word', 'young_freq', 'old_freq', 'log_ratio', 'p_value']].reset_index(drop=True)\n",
    "print(old_table)\n",
    "\n",
    "# Save tables to CSV\n",
    "young_table.to_csv('young_distinctive_words.csv', index=False)\n",
    "old_table.to_csv('old_distinctive_words.csv', index=False)\n",
    "\n",
    "print(\"\\nSaved distinctive words lists to CSV files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726acaf1",
   "metadata": {},
   "source": [
    "## 6. Create Word Cloud Visualizations\n",
    "\n",
    "Finally, let's create word clouds for each age group to visualize their distinctive vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6f7e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Create word clouds based on distinctiveness\n",
    "def create_age_wordcloud(words_df, title, output_filename):\n",
    "    \"\"\"Create and save a wordcloud for distinctive words\"\"\"\n",
    "    # Create a dictionary of word frequencies scaled by log_ratio\n",
    "    freq_dict = dict(zip(\n",
    "        words_df['word'], \n",
    "        words_df['total_freq'] * np.abs(words_df['log_ratio'])\n",
    "    ))\n",
    "    \n",
    "    # Generate word cloud\n",
    "    wordcloud = WordCloud(\n",
    "        width=800, height=400, \n",
    "        background_color='white',\n",
    "        max_words=100,\n",
    "        colormap='Blues' if 'young' in output_filename.lower() else 'Reds',\n",
    "        contour_width=1, contour_color='steelblue'\n",
    "    ).generate_from_frequencies(freq_dict)\n",
    "    \n",
    "    # Display the word cloud\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title, fontsize=18)\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.savefig(output_filename, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Create word clouds for each age group\n",
    "create_age_wordcloud(\n",
    "    young_distinctive, \n",
    "    'Words Distinctive of Young Speakers',\n",
    "    'young_distinctive_wordcloud.png'\n",
    ")\n",
    "\n",
    "create_age_wordcloud(\n",
    "    old_distinctive, \n",
    "    'Words Distinctive of Old Speakers',\n",
    "    'old_distinctive_wordcloud.png'\n",
    ")\n",
    "\n",
    "print(\"Word clouds created and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41a271f",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've created several visualizations to explore the differences in word usage between young and old speakers:\n",
    "\n",
    "1. **Interactive Scattertext Plot**: An HTML visualization that provides an interactive exploration of word differences.\n",
    "\n",
    "2. **Static Word Usage Scatter Plot**: A plot showing the most distinctive words for each age group, with word frequency in young speakers on the x-axis and frequency in old speakers on the y-axis.\n",
    "\n",
    "3. **Word Frequency Tables**: Tables listing the top 20 most distinctive words for each age group, along with their frequencies and statistical significance.\n",
    "\n",
    "4. **Word Clouds**: Visual representations of the distinctive vocabulary for each age group.\n",
    "\n",
    "These visualizations reveal interesting patterns in how language use differs between age groups in the BNC2014 corpus. The scatter plot in particular provides a clear visualization of which words are more characteristic of young vs. old speakers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
